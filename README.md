# openai4rs

[![Crates.io](https://img.shields.io/crates/v/openai4rs)](https://crates.io/crates/openai4rs)
[![Documentation](https://docs.rs/openai4rs/badge.svg)](https://docs.rs/openai4rs)
[![License](https://img.shields.io/crates/l/openai4rs)](LICENSE)

ä¸€ä¸ªåŸºäº `tokio` å’Œ `reqwest` çš„å¼‚æ­¥ Rust crateï¼Œç”¨äºä¸éµå¾ª OpenAI è§„èŒƒçš„å¤§æ¨¡å‹ä¾›åº”å•†è¿›è¡Œäº¤äº’ã€‚

## âœ¨ ç‰¹æ€§

### ğŸ—¨ï¸ Chat èŠå¤©

- âœ… æµå¼å“åº”
- âœ… å·¥å…·è°ƒç”¨
- âœ… å¤šè½®å¯¹è¯
- âœ… è§†è§‰ï¼ˆVisionï¼‰APIï¼ˆå¦‚æœæ¨¡å‹æ”¯æŒï¼‰

### ğŸ“ Completions æ–‡æœ¬è¡¥å…¨ (Legacy)

- âœ… éæµå¼å“åº”
- âœ… æµå¼å“åº”

### ğŸ—ºï¸ Embeddings è¯åµŒå…¥

- âœ… ç”Ÿæˆæ–‡æœ¬å‘é‡è¡¨ç¤º
- âœ… å•ä¸ªæˆ–å¤šä¸ªæ–‡æœ¬åŒæ—¶åµŒå…¥

### ğŸ¤– Models æ¨¡å‹ç®¡ç†

- âœ… è·å–æ¨¡å‹åˆ—è¡¨
- âœ… è·å–å•ä¸ªæ¨¡å‹ä¿¡æ¯

### ğŸ”„ HTTP è¯·æ±‚æ§åˆ¶

- âœ… å¯é…ç½®çš„é‡è¯•æ¬¡æ•°
- âœ… å¯é…ç½®çš„è¯·æ±‚è¶…æ—¶
- âœ… å¯é…ç½®çš„è¿æ¥è¶…æ—¶
- âœ… HTTP ä»£ç†æ”¯æŒ
- âœ… è‡ªå®šä¹‰ User-Agent
- âœ… å…¨å±€è¯·æ±‚å¤´
- âœ… å…¨å±€æŸ¥è¯¢å‚æ•°
- âœ… å…¨å±€è¯·æ±‚ä½“

## ğŸš€ å¿«é€Ÿå¼€å§‹

### å®‰è£…

æ·»åŠ ä¾èµ–åˆ°ä½ çš„ `Cargo.toml`ï¼š

```toml
[dependencies]
openai4rs = "0.1.9"
tokio = { version = "1.45.1", features = ["full"] }
futures = "0.3.31"
dotenvy = "0.15"
```

æˆ–ä½¿ç”¨ cargo å‘½ä»¤ï¼š

```bash
cargo add openai4rs
```

### åŸºç¡€ä½¿ç”¨

æ‰€æœ‰çš„ç«¯ç‚¹éƒ½æä¾›äº†å„è‡ªçš„å‚æ•°æ„å»ºå™¨

ä¾‹å¦‚ ChatParam, EmbeddingsParam

```rust
use dotenvy::dotenv;
use openai4rs::*;

#[tokio::main]
async fn main() -> Result<(), Box<dyn std::error::Error>> {
    dotenv().ok();
    let client = OpenAI::from_env()?;

    let model = "Qwen/Qwen3-235B-A22B-Instruct-2507";
    let messages = vec![
        system!("You are a helpful assistant."),
        user!("Introduce the Rust programming language in one sentence."),
    ];

    let request = ChatParam::new(model, &messages);

    println!("Sending request to model: {}...", model);

    let response = client.chat().create(request).await?;

    if let Some(content) = response.content() {
        println!("\nResponse:\n{}", content);
    } else {
        println!("\nNo content in response.");
    }

    Ok(())
}

```

## ğŸ“š æ ¸å¿ƒç”¨æ³•

### **ğŸ—¨ï¸ Chat èŠå¤©**

#### æµå¼èŠå¤©

å®æ—¶æ¥æ”¶å“åº”å†…å®¹ï¼Œé€‚åˆéœ€è¦é€æ­¥æ˜¾ç¤ºçš„åœºæ™¯ï¼š

```rust
use std::io::{self, Write};

use dotenvy::dotenv;
use futures::StreamExt;
use openai4rs::*;

#[tokio::main]
async fn main() -> Result<(), Box<dyn std::error::Error>> {
    dotenv().ok();
    let client = OpenAI::from_env()?;

    let model = "Qwen/Qwen3-235B-A22B-Instruct-2507";
    let messages = vec![
        system!(content: "You are a helpful assistant."),
        user!(content: "Introduce the Rust programming language in one sentence."),
    ];

    let request = ChatParam::new(model, &messages);

    println!("Sending request to model: {}...", model);

    let mut stream = client.chat().create_stream(request).await?;
    let mut first_content = true;

    while let Some(chunk_result) = stream.next().await {
        match chunk_result {
            Ok(chunk) => {
                if chunk.has_content() {
                    if first_content {
                        println!("\n========Response========");
                        first_content = false;
                    }
                    if let Some(content) = chunk.content() {
                        print!("{}", content);
                        io::stdout().flush()?;
                    }
                }
            }
            Err(e) => {
                eprintln!("\nAn error occurred during streaming: {}", e);
                break;
            }
        }
    }
    println!();

    Ok(())
}
```

#### ğŸ”§ å·¥å…·è°ƒç”¨

è®©æ¨¡å‹èƒ½å¤Ÿè°ƒç”¨å¤–éƒ¨å·¥å…·æ¥å¢å¼ºåŠŸèƒ½ï¼š

```rust
use dotenvy::dotenv;
use openai4rs::*;

// æ¨¡æ‹Ÿè·å–å¤©æ°”æ•°æ®çš„å‡½æ•°
fn get_current_weather(location: &str, unit: Option<&str>) -> String {
    // åœ¨å®é™…åº”ç”¨ä¸­ï¼Œè¿™å°†è°ƒç”¨å¤–éƒ¨å¤©æ°”APIã€‚
    let unit = unit.unwrap_or("celsius");
    format!(
        "The current weather in {} is 22 degrees {}.",
        location, unit
    )
}

#[tokio::main]
async fn main() -> Result<(), Box<dyn std::error::Error>> {
    dotenv().ok();
    let client = OpenAI::from_env()?;

    let model = "Qwen/Qwen3-235B-A22B-Instruct-2507";

    // 1. å®šä¹‰å·¥å…·ï¼ˆå‡½æ•°ï¼‰
    let weather_tool_params = Parameters::object()
        .property(
            "location",
            Parameters::string()
                .description("The city and state, e.g. San Francisco, CA")
                .build(),
        )
        .property(
            "unit",
            Parameters::string()
                .description("The unit of temperature, e.g. celsius or fahrenheit")
                .build(),
        )
        .require("location")
        .build()?;

    let weather_tool = ChatCompletionToolParam::function(
        "get_current_weather",
        "Get the current weather in a given location",
        weather_tool_params,
    );

    // 2. åˆ›å»ºåˆå§‹æ¶ˆæ¯å’Œè¯·æ±‚
    let messages = vec![
        system!(content = "You are a helpful assistant."),
        user!(content = "What's the weather like in Boston today?"),
    ];

    let request = ChatParam::new(model, &messages)
        .tools(vec![weather_tool])
        .tool_choice(ToolChoice::Auto);

    println!("Sending request to model: {}...", model);

    let response = client.chat().create(request).await?;
    println!("Initial response: {:#?}", response);

    // 3. æ£€æŸ¥æ¨¡å‹æ˜¯å¦éœ€è¦è°ƒç”¨å·¥å…·
    if response.has_tool_calls() {
        println!("\nModel wants to call a tool.");
        let tool_calls = response.tool_calls().unwrap();

        // ä¸ºç®€å•èµ·è§ï¼Œæˆ‘ä»¬åªå¤„ç†ç¬¬ä¸€ä¸ªå·¥å…·è°ƒç”¨
        if let Some(tool_call) = tool_calls.first() {
            let function_name = &tool_call.function.name;
            let arguments_str = &tool_call.function.arguments;

            if function_name == "get_current_weather" {
                let args: serde_json::Value = serde_json::from_str(arguments_str)?;
                let location = args["location"].as_str().unwrap_or("Unknown");
                let unit = args["unit"].as_str();

                println!(
                    "Calling function '{}' with arguments: location='{}', unit='{:?}'",
                    function_name, location, unit
                );

                // 4. è°ƒç”¨å‡½æ•°å¹¶è·å–ç»“æœ
                let function_result = get_current_weather(location, unit);
                println!("Function result: {}", function_result);

                // 5. å°†å‡½æ•°ç»“æœå‘é€å›æ¨¡å‹
                let mut new_messages = messages.clone();
                new_messages.push(response.first_choice_message().unwrap().clone().into());
                new_messages.push(tool!(
                    tool_call_id = tool_call.function.id.clone(),
                    content = function_result
                ));

                let follow_up_request = ChatParam::new(model, &new_messages);

                let final_response = client.chat().create(follow_up_request).await?;
                if let Some(content) = final_response.content() {
                    println!("\nFinal Assistant Response:\n{}", content);
                }
            }
        }
    } else {
        // If no tool call, just print the content
        if let Some(content) = response.content() {
            println!("\nAssistant Response:\n{}", content);
        }
    }

    Ok(())
}

```

#### ğŸ§  å¤šè½®å¯¹è¯

ç»´æŠ¤ä¸€ä¸ªå…·æœ‰ä¸Šä¸‹æ–‡çš„å¤šè½®å¯¹è¯ï¼š

```rust
use dotenvy::dotenv;
use openai4rs::*;
use std::io::{Write, stdin, stdout};

#[tokio::main]
async fn main() -> Result<(), Box<dyn std::error::Error>> {
    dotenv().ok();
    let client = OpenAI::from_env()?;

    let model = "Qwen/Qwen3-235B-A22B-Instruct-2507";
    let mut messages = vec![system!(content: "You are a helpful assistant.")];

    loop {
        print!("You: ");
        stdout().flush()?;
        let mut user_input = String::new();
        stdin().read_line(&mut user_input)?;
        let user_input = user_input.trim();

        if user_input.eq_ignore_ascii_case("exit") {
            println!("Goodbye!");
            break;
        }

        messages.push(user!(content: user_input));

        let request = ChatParam::new(model, &messages);

        let response = client.chat().create(request).await?;
        if let Some(content) = response.content() {
            println!("Assistant: {}\n", content);
            messages.push(assistant!(content));
        } else {
            println!("Assistant: No response.\n");
        }
    }

    Ok(())
}

```

### **ğŸ—ºï¸ Embeddings è¯åµŒå…¥**

ç”Ÿæˆæ–‡æœ¬çš„å‘é‡è¡¨ç¤ºï¼Œç”¨äºæœç´¢ã€èšç±»å’Œå…¶ä»–æœºå™¨å­¦ä¹ ä»»åŠ¡ï¼š

```rust
use dotenvy::dotenv;
use openai4rs::*;

#[tokio::main]
async fn main() -> Result<(), Box<dyn std::error::Error>> {
    dotenv().ok();
    let client = OpenAI::from_env()?;

    // 1. å•ä¸ªæ–‡æœ¬åµŒå…¥
    let request = EmbeddingsParam::new("text-embedding-ada-002", "Hello, world!");
    let response = client.embeddings().create(request).await?;
    println!("Generated {} embedding(s)", response.len());
    if let Some(embedding) = response.get_embedding(0) {
        println!("Embedding dimensions: {}", embedding.dimensions());
    }

    // 2. å¤šä¸ªæ–‡æœ¬åµŒå…¥
    let texts = vec!["Hello, world!", "How are you?", "Rust is awesome!"];
    let request = EmbeddingsParam::new("text-embedding-ada-002", texts);
    let response = client.embeddings().create(request).await?;
    println!("Generated {} embeddings", response.len());
    for (i, embedding) in response.embeddings().iter().enumerate() {
        println!("Embedding {}: {} dimensions", i, embedding.dimensions());
    }

    // 3. è·å–åµŒå…¥å‘é‡
    let embedding_vectors = response.embedding_vectors();
    println!("First vector length: {}", embedding_vectors[0].len());

    Ok(())
}

```

### **ğŸ”§ é«˜çº§é…ç½®**

#### å®¢æˆ·ç«¯é…ç½®

```rust
use std::time::Duration;

use dotenvy::dotenv;
use openai4rs::*;

#[tokio::main]
async fn main() -> Result<(), Box<dyn std::error::Error>> {
    dotenv().ok();

    // è·å–ç¯å¢ƒå˜é‡
    let api_key = std::env::var("OPENAI_API_KEY")?;
    let base_url = std::env::var("OPENAI_BASE_URL")?;
    // 1. åŸºç¡€å®¢æˆ·ç«¯
    let basic_client = OpenAI::new(&api_key, &base_url);

    // 2. å…·æœ‰è‡ªå®šä¹‰åŸºç¡€URLçš„å®¢æˆ·ç«¯ï¼ˆä¾‹å¦‚ï¼Œç”¨äºä»£ç†æˆ–ä¸åŒä¾›åº”å•†ï¼‰
    let _custom_base_url_client = Config::builder()
        .api_key(&api_key)
        .base_url(&base_url) // æ›¿æ¢ä¸ºæ‚¨çš„è‡ªå®šä¹‰åŸºç¡€URL
        .build_openai()?;

    // 3. å¸¦ä»£ç†çš„å®¢æˆ·ç«¯
    let proxy_config = Config::builder()
        .api_key(&api_key)
        .base_url(&base_url)
        .proxy("http://proxy.example.com:8080")
        .build()?;
    let _proxy_client = OpenAI::with_config(proxy_config);

    // 4. å¸¦è‡ªå®šä¹‰è¶…æ—¶çš„å®¢æˆ·ç«¯
    let timeout_config = Config::builder()
        .api_key(&api_key)
        .base_url(&base_url)
        .timeout(Duration::from_secs(120))
        .build()?;
    let _timeout_client = OpenAI::with_config(timeout_config);

    // ä¸ºäº†æ¼”ç¤ºï¼Œæˆ‘ä»¬å°†ä½¿ç”¨åŸºç¡€å®¢æˆ·ç«¯å‘å‡ºç®€å•è¯·æ±‚ã€‚
    // åœ¨å®é™…åº”ç”¨ä¸­ï¼Œæ‚¨åº”ä½¿ç”¨æœ€é€‚åˆæ‚¨éœ€æ±‚çš„å®¢æˆ·ç«¯ã€‚

    let model = "Qwen/Qwen3-235B-A22B-Instruct-2507";
    let messages = vec![user!(content: "Ping to check if the client is working.")];
    let request = ChatParam::new(model, &messages);

    println!("Testing basic client...");
    match basic_client.chat().create(request).await {
        Ok(response) => {
            if let Some(content) = response.content() {
                println!("Success: {}", content);
            }
        }
        Err(e) => {
            eprintln!("Error with basic client: {}", e);
        }
    }

    Ok(())
}

```

## ğŸ“– è¿è¡Œç¤ºä¾‹

æŸ¥çœ‹ [examples](examples/) ç›®å½•è·å–æ›´å¤šä½¿ç”¨ç¤ºä¾‹ï¼š

- [01. åŸºç¡€èŠå¤©](examples/01_simple_chat.rs)
- [02. æµå¼å“åº”](examples/02_streaming_chat.rs)
- [03. å¤šè½®å¯¹è¯](examples/03_multi_turn_chat.rs)
- [04. å·¥å…·è°ƒç”¨](examples/04_tool_use.rs)
- [05. å®¢æˆ·ç«¯é…ç½®](examples/05_client_configuration.rs)
- [06. è§†è§‰ï¼ˆVisionï¼‰API](examples/06_vision.rs) (å¦‚æœæ¨¡å‹æ”¯æŒ)
- [07. æ€ç»´æ¨¡å‹ï¼ˆThinking Modelï¼‰](examples/07_thinking_model.rs) (å¦‚æœæ¨¡å‹æ”¯æŒå¤æ‚æ¨ç†)
- [08. è¯åµŒå…¥ï¼ˆEmbeddingsï¼‰](examples/08_embeddings_example.rs)

ä½ å¯ä»¥é€šè¿‡ä»¥ä¸‹å‘½ä»¤è¿è¡Œç¤ºä¾‹ï¼š

```bash
# è®¾ç½®ç¯å¢ƒå˜é‡
export OPENAI_API_KEY=your_api_key
export OPENAI_BASE_URL=your_base_url # å¯é€‰, é»˜è®¤ä¸º https://api.openai.com/v1

# è¿è¡Œç¤ºä¾‹
cargo run --example 01_simple_chat
cargo run --example 02_streaming_chat
# ... å…¶ä»–ç¤ºä¾‹
```

## ğŸ“„ è®¸å¯è¯

æœ¬é¡¹ç›®é‡‡ç”¨ [Apache-2.0 è®¸å¯è¯](LICENSE)ã€‚

## ğŸ”— ç›¸å…³é“¾æ¥

- [æ–‡æ¡£](https://docs.rs/openai4rs)
- [Crates.io](https://crates.io/crates/openai4rs)
- [GitHub ä»“åº“](https://github.com/zhangzhenxiang666/openai4rs)
- [é—®é¢˜åé¦ˆ](https://github.com/zhangzhenxiang666/openai4rs/issues)
