use super::types::*;
use crate::common::types::ServiceTier;
use derive_builder::Builder;
use serde::Serialize;
use std::collections::HashMap;

/// Parameters for creating a model response for chat conversations.
///
/// This struct represents the request parameters for OpenAI's Chat Completions API,
/// supporting text generation, vision, and audio capabilities. Parameter support
/// may vary depending on the model used, particularly for newer reasoning models.
#[derive(Debug, Clone, Serialize, Builder)]
#[builder(name = "RequestParamsBuilder")]
#[builder(derive(Debug))]
#[builder(pattern = "owned")]
#[builder(setter(strip_option))]
pub struct RequestParams<'a> {
    /// Model ID used to generate the response, like `gpt-4o` or `o1`.
    ///
    /// OpenAI offers a wide range of models with different capabilities,
    /// performance characteristics, and price points.
    pub model: &'a str,

    /// A list of messages comprising the conversation so far.
    ///
    /// Depending on the model you use, different message types (modalities)
    /// are supported, like text, images, and audio.
    pub messages: &'a Vec<ChatCompletionMessageParam>,

    /// Number between -2.0 and 2.0. Positive values penalize new tokens based on their
    /// existing frequency in the text so far, decreasing the model's likelihood to
    /// repeat the same line verbatim.
    #[builder(default)]
    #[serde(skip_serializing_if = "Option::is_none")]
    pub frequency_penalty: Option<f32>,

    /// Modify the likelihood of specified tokens appearing in the completion.
    ///
    /// Accepts a JSON object that maps tokens (specified by their token ID in the
    /// tokenizer) to an associated bias value from -100 to 100. The bias is added
    /// to the logits generated by the model prior to sampling.
    #[builder(default)]
    #[serde(skip_serializing_if = "Option::is_none")]
    pub logit_bias: Option<HashMap<String, i32>>,

    /// Whether to return log probabilities of the output tokens or not.
    ///
    /// If true, returns the log probabilities of each output token returned
    /// in the `content` of `message`.
    #[builder(default)]
    #[serde(skip_serializing_if = "Option::is_none")]
    pub logprobs: Option<bool>,

    /// Output types that you would like the model to generate.
    ///
    /// Most models are capable of generating text, which is the default: `["text"]`.
    /// The `gpt-4o-audio-preview` model can also generate audio. To request both
    /// text and audio responses, use: `["text", "audio"]`.
    #[builder(default)]
    #[serde(skip_serializing_if = "skip_if_option_vec_empty")]
    pub modalities: Option<Vec<Modality>>,

    /// An upper bound for the number of tokens that can be generated for a completion,
    /// including visible output tokens and reasoning tokens.
    #[builder(default)]
    #[serde(skip_serializing_if = "Option::is_none")]
    pub max_completion_tokens: Option<i32>,

    /// The maximum number of tokens that can be generated in the chat completion.
    ///
    /// This value can be used to control costs for text generated via API.
    /// with o1 series models.
    #[builder(default)]
    #[serde(skip_serializing_if = "Option::is_none")]
    #[deprecated(note = "Use `max_completion_tokens` instead")]
    pub max_tokens: Option<i32>,

    /// Set of 16 key-value pairs that can be attached to an object.
    ///
    /// This can be useful for storing additional information about the object
    /// in a structured format. Keys have a maximum length of 64 characters,
    /// values have a maximum length of 512 characters.
    #[builder(default)]
    #[serde(skip_serializing_if = "Option::is_none")]
    pub metadata: Option<HashMap<String, String>>,

    /// Whether to enable parallel function calling during tool use.
    #[builder(default)]
    #[serde(skip_serializing_if = "Option::is_none")]
    pub parallel_tool_calls: Option<bool>,

    /// Number between -2.0 and 2.0. Positive values penalize new tokens based on
    /// whether they appear in the text so far, increasing the model's likelihood
    /// to talk about new topics.
    #[builder(default)]
    #[serde(skip_serializing_if = "Option::is_none")]
    pub presence_penalty: Option<f32>,

    /// How many chat completion choices to generate for each input message.
    ///
    /// Note that you will be charged based on the number of generated tokens
    /// across all of the choices. Keep `n` as `1` to minimize costs.
    #[builder(default)]
    #[serde(skip_serializing_if = "Option::is_none")]
    pub n: Option<i32>,

    /// An alternative to sampling with temperature, called nucleus sampling.
    ///
    /// The model considers the results of the tokens with top_p probability mass.
    /// So 0.1 means only the tokens comprising the top 10% probability mass are considered.
    /// We generally recommend altering this or `temperature` but not both.
    #[builder(default)]
    #[serde(skip_serializing_if = "Option::is_none")]
    pub top_p: Option<f32>,

    /// If set to true, the model response data will be streamed to the client
    /// as it is generated using server-sent events.
    ///
    /// See the streaming responses guide for more information on how to handle
    /// the streaming events.
    #[builder(default)]
    #[serde(skip_serializing_if = "Option::is_none")]
    pub stream: Option<bool>,

    /// Whether or not to store the output of this chat completion request for use
    /// in model distillation or evals products.
    #[builder(default)]
    #[serde(skip_serializing_if = "Option::is_none")]
    pub store: Option<bool>,

    /// Up to 4 sequences where the API will stop generating further tokens.
    ///
    /// The returned text will not contain the stop sequence.
    /// **Note**: This field name appears to be `send` in your struct but should likely be `stop`.
    #[builder(default)]
    #[serde(skip_serializing_if = "Option::is_none")]
    pub send: Option<i32>,

    /// What sampling temperature to use, between 0 and 2.
    ///
    /// Higher values like 0.8 will make the output more random, while lower values
    /// like 0.2 will make it more focused and deterministic. We generally recommend
    /// altering this or `top_p` but not both.
    #[builder(default)]
    #[serde(skip_serializing_if = "Option::is_none")]
    pub temperature: Option<f32>,

    /// A unique identifier representing your end-user, which can help OpenAI
    /// to monitor and detect abuse.
    #[builder(default)]
    #[serde(skip_serializing_if = "Option::is_none")]
    pub user: Option<String>,

    /// An integer between 0 and 20 specifying the number of most likely tokens
    /// to return at each token position, each with an associated log probability.
    ///
    /// `logprobs` must be set to `true` if this parameter is used.
    #[builder(default)]
    #[serde(skip_serializing_if = "Option::is_none")]
    pub top_logprobs: Option<i32>,

    /// Static predicted output content, such as the content of a text file
    /// that is being regenerated.
    #[builder(default)]
    #[serde(skip_serializing_if = "Option::is_none")]
    pub prediction: Option<ChatCompletionPredictionContentParam>,

    /// **o-series models only** - Constrains effort on reasoning for reasoning models.
    ///
    /// Currently supported values are `low`, `medium`, and `high`. Reducing reasoning
    /// effort can result in faster responses and fewer tokens used on reasoning in a response.
    #[builder(default)]
    #[serde(skip_serializing_if = "Option::is_none")]
    pub reasoning_effort: Option<ReasoningEffort>,

    /// Specifies the latency tier to use for processing the request.
    ///
    /// This parameter is relevant for customers subscribed to the scale tier service.
    /// - If set to 'auto', and the Project is Scale tier enabled, the system will
    ///   utilize scale tier credits until they are exhausted.
    /// - If set to 'default', the request will be processed using the default service
    ///   tier with a lower uptime SLA and no latency guarantee.
    #[builder(default)]
    #[serde(skip_serializing_if = "Option::is_none")]
    pub service_tier: Option<ServiceTier>,

    /// A list of tools the model may call. Currently, only functions are supported as a tool.
    ///
    /// Use this to provide a list of functions the model may generate JSON inputs for.
    /// A max of 128 functions are supported.
    #[builder(default)]
    #[serde(skip_serializing_if = "skip_if_option_vec_empty")]
    pub tools: Option<Vec<ChatCompletionToolParam>>,

    /// Controls which (if any) tool is called by the model.
    ///
    /// - `none` means the model will not call any tool and instead generates a message.
    /// - `auto` means the model can pick between generating a message or calling one or more tools.
    /// - `required` means the model must call one or more tools.
    /// - Specifying a particular tool forces the model to call that tool.
    ///
    /// `none` is the default when no tools are present. `auto` is the default if tools are present.
    #[builder(default)]
    #[serde(skip_serializing_if = "Option::is_none")]
    pub tool_choice: Option<ToolChoice>,

    /// Send extra headers with the request.
    ///
    /// This field is not serialized in the request body.
    #[builder(default)]
    #[serde(skip_serializing)]
    pub extra_headers: Option<HashMap<String, serde_json::Value>>,

    /// Add additional query parameters to the request.
    ///
    /// This field is not serialized in the request body.
    #[builder(default)]
    #[serde(skip_serializing)]
    pub extra_query: Option<HashMap<String, serde_json::Value>>,

    /// Add additional JSON properties to the request.
    ///
    /// This field is not serialized in the request body.
    #[builder(default)]
    #[serde(skip_serializing)]
    pub extra_body: Option<HashMap<String, serde_json::Value>>,

    /// HTTP request retry count, overrides the client's global setting.
    ///
    /// This field is not serialized in the request body.
    #[builder(default)]
    #[serde(skip_serializing)]
    pub retry_count: Option<u32>,

    /// HTTP request timeout in seconds, overrides the client's global setting.
    ///
    /// This field is not serialized in the request body.
    #[builder(default)]
    #[serde(skip_serializing)]
    pub timeout_seconds: Option<u64>,

    /// HTTP request User-Agent, overrides the client's global setting.
    ///
    /// This field is not serialized in the request body.
    #[builder(default)]
    #[serde(skip_serializing)]
    pub user_agent: Option<String>,
}

pub fn chat_request<'a>(
    model: &'a str,
    messages: &'a Vec<ChatCompletionMessageParam>,
) -> RequestParamsBuilder<'a> {
    RequestParamsBuilder::create_empty()
        .model(model)
        .messages(messages)
}

pub trait IntoRequestParams<'a> {
    fn into_request_params(self) -> RequestParams<'a>;
}

impl<'a> IntoRequestParams<'a> for RequestParams<'a> {
    fn into_request_params(self) -> RequestParams<'a> {
        self
    }
}

impl<'a> IntoRequestParams<'a> for RequestParamsBuilder<'a> {
    fn into_request_params(self) -> RequestParams<'a> {
        self.build().unwrap()
    }
}

fn skip_if_option_vec_empty<T>(opt: &Option<Vec<T>>) -> bool
where
    T: std::fmt::Debug,
{
    opt.as_ref().is_none_or(Vec::is_empty)
}
